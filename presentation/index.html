<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2023-01-18">
  <title>Norse: Machine Learning with SNN</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link href='css/fonts.css' rel='stylesheet'>
  <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
  <link rel="stylesheet" href="css/white.css" id="theme">
  <link rel="stylesheet" href="https://unpkg.com/tachyons@4.12.0/css/tachyons.min.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <img src="https://raw.githubusercontent.com/norse/norse/master/logo.png"/>
  <h1 class="title">Norse: Machine Learning with SNN</h1>
<div>
<div class="flex"> 
<div class="fl pr5">
  <p class="white-70 f1">Christian Pehle</p>
  <p class="white-70 f2">Heidelberg University</p>
  <a href="mailto:christian.pehle@kip.uni-heidelberg.de" class="link dim f2">christian.pehle@kip.uni-heidelberg.de</a>
</div>
<div class="fl pr5">
  <p class="white-70 f1">Jens E. Pedersen</p>
  <p class="white-70 f2">KTH Stockholm</p>
  <a href="mailto:jeped@kth.se" class="link dim f2">jeped@kth.se</a>
</div>
</div>
</p>
</div>
  <p class="f1 gray">January 18, 2023</p>
</section>

<section id="what-is-norse" class="slide level2">
<h2>What is Norse?</h2>
<div class="fragment">
<div class="f1 green">
<p>Spiking Neuron Primitives implemented for PyTorch</p>
</div>
</div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l mt5 fragment">
<div class="fl w-33 tc calibre normal f2 mt3">
<div class="mw7">
<p><img data-src="triangle_v.png" class="h5 rotate-90" /></p>
<p>Blazing fast</p>
</div>
</div>
<div class="fl w-33 tc calibre normal f2 mt3 fragment">
<div class="mw7">
<p><img data-src="square_g.png" class="h5 rotate-45" /></p>
<p>Feels amazing to use</p>
</div>
</div>
<div class="fl w-33 tc calibre normal f2 mt3 fragment">
<div class="mw7">
<p><img data-src="circle_t.png" class="h5 rotate-90" /></p>
<p>Quick to get started</p>
</div>
</div>
</div>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="triangle_v.png" class="h5 rotate-90" /></p>
<p>Blazing fast</p>
</div>
<div class="fl w-75 tc calibre normal f1">
<div class="lh-copy mt5 fl pl5">
<ul>
<li class="fragment">PyTorch already takes care of most of the
work.</li>
<li class="fragment">Implementation is performance aware.</li>
<li class="fragment">Custom code for selected operations.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="square_g.png" class="h5 rotate-45" /></p>
<p>Feels amazing to use</p>
</div>
<div class="fl w-75 tc calibre normal f1">
<div class="lh-copy mt5 fl pl5">
<ul>
<li class="fragment">Complexity is revealed gradually.</li>
<li class="fragment">Sensible defaults.</li>
<li class="fragment">Easily customisable and extensible.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="circle_t.png" class="h5 rotate-90" /></p>
<p>Quick to get started</p>
</div>
<div class="fl w-75 tc calibre normal f1">
<div class="fl ph5">
<p>Install the PIP or conda package:</p>
<p><code>pip install norse</code></p>
</div>
<div class="lh-copy mt5 fl ph5">
<ul>
<li class="fragment">Try out one of the <a
href="https://norse.github.io/notebooks/">jupyter notebooks</a>.</li>
<li class="fragment">Read our <a
href="https://norse.github.io/norse/">interactive
documentation</a>.</li>
<li class="fragment">End-to-end <a
href="https://github.com/norse/norse/tree/main/norse/task">example
tasks</a>.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="event-based-processing-with-norse" class="slide level2">
<h2>Event-based processing with Norse</h2>
<ul>
<li>Asynchronous and parallel <span class="fragment">- perfect for
neuromorphics</span></li>
</ul>
<p><img
data-src="https://upload.wikimedia.org/wikipedia/commons/2/26/Event_camera_comparison.jpg" /></p>
</section>
<section id="event-based-neuron-models" class="slide level2">
<h2>Event-based neuron models</h2>
<div class="fl w-50 tc calibre normal f1 bg-washed-blue">
<p><img style="height:120px;" src="2209_event_norse.png"/></p>
</div>
<div class="fl w-10 tc calibre normal f1">
<p><br/></p>
</div>
<div class="fl w-40 tc calibre normal f1 bg-washed-blue">
<p><img style="height:120px;" src="edge_detector.png"/></p>
</div>
<div class="fl w-100 tc calibre normal f1 fragment">
<p><img
data-src="https://github.com/norse/aestream/raw/main/example/usb_edgedetection.gif" /></p>
</div>
</section>
<section id="event-based-edge-detector" class="slide level2">
<h2>Event-based edge detector</h2>
<div class="fl w-20 tc calibre normal f1">
<p><img
data-src="https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides.gif" /></p>
<!-- Source: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md -->
</div>
<div class="fl w-10 tc calibre normal f1">
<p><br/></p>
</div>
<div class="fl w-70 tc calibre normal f1 fragment">
<p><br/></p>
<pre data-id="code-animation"><code class="python" data-line-numbers="1|2|4,7|5|6|4-7">import torch
from norse.torch import LIFRefracCell

model = nn.Sequential(
    LIFRefracCell(),
    torch.nn.Conv2d(1, 1, 5),
)</code></pre>
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<video width="70%" controls loop>
<source src="https://jepedersen.dk/slides/202209_neurotech/2209-edge-detection.mp4" type="video/mp4">
</video>
</section>
<section id="event-based-streaming-with-norse" class="slide level2">
<h2>Event-based streaming with Norse</h2>
<ul>
<li>Real-time streaming with AEStream
<ul>
<li><code>with FileInput(...) as file: ...</code></li>
</ul></li>
</ul>
<p><br/></p>
<div class="fragment">
<ul>
<li>Real-time spiking neuron dynamics with Norse
<ul>
<li><code>lif_network(file.read())</code></li>
</ul></li>
</ul>
</div>
<p><br/></p>
<div class="fragment">
<ul>
<li>Future projects
<ul>
<li>Control systems</li>
<li>Plasticity</li>
<li>Meta learning</li>
</ul></li>
</ul>
</div>
</section>
<section id="deep-learning-with-norse" class="slide level2">
<h2>Deep Learning with Norse</h2>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l mt5">
<div class="fl w-33 tc calibre normal f2 mt3 fragment">
<div class="mw7">
<p><img data-src="triangle_c.png" class="h5 rotate-90" /></p>
<p>Familiar to ML researchers</p>
</div>
</div>
<div class="fl w-33 tc calibre normal f2 mt3 fragment">
<div class="mw7">
<p><img data-src="hexagon_r.png" class="h5 rotate-90" /></p>
<p>Compose primitives freely</p>
</div>
</div>
<div class="fl w-33 tc calibre normal f2 mt3 fragment">
<div class="mw7">
<p><img data-src="circle_y.png" class="h5 rotate-90" /></p>
<p>Extensible by everyone</p>
</div>
</div>
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal">
<p><img data-src="triangle_c.png" class="rotate-90 h5" /></p>
<p>Familiar to ML researchers</p>
</div>
<div class="fl w-75 tc calibre normal f1">
<div class="lh-copy mt5 fl ph5">
<ul>
<li class="fragment">Build models like you would in PyTorch.</li>
<li class="fragment">Use dataloaders and training frameworks as is.</li>
<li class="fragment">Mix ANN and SNN layers.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="section-7" class="slide level2" data-auto-animate="">
<h2 data-auto-animate=""></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="hexagon_r.png" class="h5 rotate-90" /></p>
<p>Compose primitives freely</p>
</div>
<div class="fl w-75 h-100 tl normal f1 tc">
<div class="ph5 fragment tl">
<p>Convolutional Neural Network</p>
</div>
<div class="fragment">
<pre data-id="code-animation"><code class="python" data-line-numbers="1|6-17|9,12,16">import torch, torch.nn as nn





model = nn.Sequential(
    nn.Conv2d(1, 20, 5, 1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(20, 50, 5, 1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Flatten(),
    nn.Linear(800, 10)
    nn.Identity()
)</code></pre>
</div>
</div>
</div>
</section>
<section id="section-8" class="slide level2" data-auto-animate="">
<h2 data-auto-animate=""></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="hexagon_r.png" class="h5 rotate-90" /></p>
<p>Compose primitives freely</p>
</div>
<div class="fl w-75 h-100 tl normal f1 tc">
<div class="ph5 tl">
<p>Convolutional Spiking Neural Network</p>
</div>
<pre data-id="code-animation"><code class="python" data-line-numbers="3-5,9,12,16,7,17|8,10,11,13,14,15|9,12,16">import torch, torch.nn as nn

from norse.torch import LICell
from norse.torch import LIFCell
from norse.torch import SequentialState

model = SequentialState(
    nn.Conv2d(1, 20, 5, 1),
    LIFCell(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(20, 50, 5, 1),
    LIFCell(),
    nn.MaxPool2d(2, 2),
    nn.Flatten(),
    nn.Linear(800, 10),
    LICell(),
)</code></pre>
</div>
</div>
</section>
<section id="section-9" class="slide level2" data-auto-animate="">
<h2 data-auto-animate=""></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="hexagon_r.png" class="h5 rotate-90" /></p>
<p>Compose primitives freely</p>
</div>
<div class="fl w-75 h-100 tl normal f1 tc">
<div class="ph5 tl">
<p>Convolutional Spiking Neural Network</p>
</div>
<pre data-id="code-animation"><code class="python" data-line-numbers="4,9,12">import torch, torch.nn as nn

from norse.torch import LICell
from norse.torch import AdexCell
from norse.torch import SequentialState, Lift

model = SequentialState(
    nn.Conv2d(1, 20, 5, 1),
    AdexCell(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(20, 50, 5, 1),
    AdexCell(),
    nn.MaxPool2d(2, 2),
    nn.Flatten(),
    nn.Linear(800, 10),
    LICell(),
)</code></pre>
</div>
</div>
</section>
<section id="section-10" class="slide level2" data-auto-animate="">
<h2 data-auto-animate=""></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="hexagon_r.png" class="h5 rotate-90" /></p>
<p>Compose primitives freely</p>
</div>
<div class="fl w-75 h-100 tl normal f1 tc">
<div class="ph5 tl">
<p>Convolutional Spiking Neural Network</p>
</div>
<pre data-id="code-animation"><code class="python" data-line-numbers="4,9,12">import torch, torch.nn as nn

from norse.torch import LICell
from norse.torch import IAFCell
from norse.torch import SequentialState

model = SequentialState(
    nn.Conv2d(1, 20, 5, 1),
    IAFCell(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(20, 50, 5, 1),
    IAFCell(),
    nn.MaxPool2d(2, 2),
    nn.Flatten(),
    nn.Linear(800, 10),
    LICell(),
)</code></pre>
</div>
</div>
</section>
<section id="section-11" class="slide level2" data-auto-animate="">
<h2 data-auto-animate=""></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="hexagon_r.png" class="h5 rotate-90" /></p>
<p>Compose primitives freely</p>
</div>
<div class="fl w-75 h-100 tl normal f1 tc">
<div class="ph5 tl">
<p>Convolutional Spiking Neural Network</p>
</div>
<pre data-id="code-animation"><code class="python" data-line-numbers="5,8,10,11,13-15|3,4,9,12,16">import torch, torch.nn as nn

from norse.torch import LI
from norse.torch import LIF
from norse.torch import Lift

model = nn.Sequential(
    Lift(nn.Conv2d(1, 20, 5, 1)),
    LIF(),
    Lift(nn.MaxPool2d(2, 2)),
    Lift(nn.Conv2d(20, 50, 5, 1)),
    LIF(),
    Lift(nn.MaxPool2d(2, 2)),
    Lift(nn.Flatten()),
    Lift(nn.Linear(800, 10)),
    LI(),
)</code></pre>
</div>
</div>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 tc calibre normal f">
<p><img data-src="circle_y.png" class="h5 rotate-90" /></p>
<p>Extensible by everyone</p>
</div>
<div class="fl tl w-75 tc normal f1">
<div class="fl lh-copy ph5 fragment">
<p>Join us on <a href="https://github.com/github/">github</a> and <a
href="https://discord.gg/7fGN359">discord</a>!</p>
</div>
<div class="lh-copy mt5 fl ph5">
<ul>
<li class="fragment">Implementation is (mostly) in Python.</li>
<li class="fragment">Simple neuron models can be added quickly.</li>
<li class="fragment"><em>Open</em> and <em>welcoming</em>
community.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="limitations" class="slide level2">
<h2>Limitations</h2>
<ul>
<li>Naive numerical integration</li>
<li>Memory limitations (BPTT)</li>
</ul>
</section>
<section id="future-work" class="slide level2">
<h2>Future Work</h2>
<ul>
<li>Gradient based online Learning Algorithms (FPTT, E-Prop)</li>
<li>Metaplasticity &amp; Neuro Inspired Models</li>
<li>Norse as interface to Neuromorphic Hardware</li>
</ul>
</section>
<section>
<section id="workshop" class="title-slide slide level1">
<h1>Workshop</h1>

</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-50 calibre tc normal f1">
<p><img data-src="triangle_g.png" class="h6 rotate-90" /></p>
<p>Event streaming</p>
</div>
<div class="fl w-50 calibre tc normal f1">
<p><img data-src="circle_y.png" class="h6" /></p>
<p>Deep Learning</p>
</div>
</div>
</section>
<section id="schedule" class="slide level2">
<h2>Schedule</h2>
<div class="fragment">
<ul>
<li>9:00 - 9:30: Presentation + Questions (&lt;- we are here)</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>9:30 - 10:15: Workshop 1/2
<ul>
<li><img data-src="triangle_g.png" class="h2 v-mid rotate-90" /> Event
streaming (Jens)</li>
<li><img data-src="circle_y.png" class="h2 v-mid" /> Deep learning
(Christian)</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>10:15 - 10:30: Break</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>10:30 - 11:15: Workshop 2/2
<ul>
<li><img data-src="triangle_g.png" class="h2 v-mid rotate-90" /> Event
streaming (Jens)</li>
<li><img data-src="circle_y.png" class="h2 v-mid" /> Deep learning
(Christian)</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>11:15 - 11:30: Q&amp;A</li>
</ul>
</div>
</section>
<section id="workshop-1-event-streaming" class="slide level2">
<h2>Workshop 1: Event streaming</h2>
</section>
<section id="goals" class="slide level2">
<h2>Goals</h2>
</section>
<section id="workshop-2-deep-learning" class="slide level2">
<h2>Workshop 2: Deep Learning</h2>
</section>
<section id="goals-1" class="slide level2">
<h2>Goals</h2>
</section></section>
<section>
<section id="lets-get-started" class="title-slide slide level1">
<h1>Let’s get started!</h1>

</section>
<section id="funding-acknowledgements" class="slide level2">
<h2>Funding &amp; Acknowledgements</h2>
<div class="flex flex-column flex-row-l flex-wrap-l">
<div class="fl w-25 calibre tc normal f1 fragment pa2">
<p><a href="https://github.com/electronicvisions">Electronic
Vision(s)</a></p>
<p><img data-src="kompass_logo-hd.svg" style="width: 100%" /></p>
</div>
<div class="fl w-25 calibre tc normal f1 fragment pa2">
<p><img data-src="ncs.png" style="width: 100%" /></p>
</div>
<div class="fl w-25 calibre tc normal f1 fragment pa2">
<p><img data-src="hbp_logo.png" style="width: 60%" /></p>
</div>
<div class="fl w-25 calibre tc normal f1 fragment pa2">
<div class="w-100">
<p><img data-src="hexagon_r.png" class="h4 rotate-90" /> <img
data-src="triangle_c.png" class="h4 rotate-90" /> <img
data-src="circle_y.png" class="h4 rotate-90" /></p>
<div class="tc gray f3">
<p>Icons by Thought Machine</p>
<p>© 2022 Thought Machine</p>
<p>Released under <a
href="https://creativecommons.org/licenses/by/4.0/">CC-BY-v4</a></p>
</div>
</div>
</div>
</div>
<div class="tl gray f3">
<p>The research has received funding from the EC Horizon 2020 Framework
Programme under Grant Agreements 785907 and 945539 (HBP) and by the
Deutsche Forschungsgemeinschaft (DFG, German Research Fundation) under
Germany’s Excellence Strategy EXC 2181/1 - 390900948 (the Heidelberg
STRUCTURES Excellence Cluster).</p>
</div>
</section></section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/highlight/highlight.js"></script>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//plugin/highlight/monokai.css">
  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: true,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        // reveal.js plugins
        plugins: [
          RevealHighlight,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
